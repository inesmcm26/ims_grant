- limpar o dataset e fazer visualizações: TODO visualizações com o target (URGENTE)

- ver se há outliers (IQR, DBSCAN  (atenção que há one hots)): em princípio não

- pesquisar acerca de -2 answers (URGENTE)

- lidar com imbalance target

- escolher 10 agoritmos

- fazer feature selection (PCA, correlation-based)

- 
- GRID SEARCH:
    - para cada algoritmo (FOR)
        - GRID SEARCH CV: para cada {set de parametros}: (FOR)
            - para cada fold: (FOR)
                - scaling (só se)
                - feature selection
                - (missing data)
                
                - treinar no treino
                - testar no treino ({lista de measures})
                - testar na validação ({lista de measures})
            - guardar a mediana/média (?) do algoritmo com esse set de parametros
            - ver se é melhor do que o anterior
        - retornar melhor set de parametros para algoritmo
        - escrever melhor set de parâmetros para algoritmo + train e val scores para um ficheiro
    - retornar mehor algoritmo

- DÚVIDAS:
1. teoricamente imputation deveria ser feita dentro de cada fold, mas isso iria aumentar exponencialmente o tempo necessário para fazer a grid search
tendo em conta que o número máximo de missing values por coluna roonda os 350, e sabendo que o dataset tem mais de 6800 observações, podemos
assumir que, por ser grande o suficiente, os dados não ficaram biased na validação?

2. Ver técnicas de imputation para cada variável: OK
    - drop de todas as observações com 5 ou mais missing values (-367 rows)
    - para colunas com menos de 100 missing values: central tendency limit approach (median e mode para cada caso consoante formato da distribuição)
    - para colunas com mais de 100 missing values: KNN com 5 neighbours OU moda

3. Questions not asked: o que fazer em cada caso? Em casos sem boa solução faz sentido simplesmente não usar a variável? OK

4. Feature engineering: não há variáveis muitooo correlacionadas à exceção de MATHARDSHIP. Vale a pena juntar? Ou é melhor apenas reduzir dimensionalidade usando PCA?
JUNTAR

5. Feature selection: Retirar variáveis muito correlacionadas entre sé. Vale a pena remover variáveis não relacionadas com o target ou é um risco?
Atenção que é uma análise univariada apenas. Provavelmente drop não é o ideal à partida.
ESCOLHER N FEATURES

PCA: a ideia é usar apenas os PC?

Para cada algoritmo fazer com todo o dataset, só com N features de correlation based/ Recursive best features e só com PCA

6. O que fazer em relação a imbalanced target? Usar class weights? (Se sim, como conciliar com a weight de cada variável?)
Usar Precision, Recall, PRC, AUC etc.? Escolhere ensembles?

class_weights SIM
Guardar várias metricas: Precision, recall, PRC, AUC etc.



-------------------------------------------
10/04/2023

Feature Selection: Escolher N features

- Tirei features altamente correlacionadas entre elas (3 featres foram dropped)

- Usar recursive feature elimination com decision tree (usa information gain! -> apropriado para features numericas e categóricas)

DÚVIDA: fazer à priori? ou fazer dentro de cada split?
- Dentro

DÚVIDA: queremos só f1 score, precision, recall, AUC certo?
weighted f1 score faz com que um peso superior seja atribuida à classe subrepresentada (pobreza máxima).
Se isto nos interessa mais do que prever class sobre representada (mínima pobreza), então talvez
faça sentido usar um weighted f1 score, ACCURACY
- F1 score, precision, recall, accuracy e std deviation

DÚVIDA: 
quando fiz impute dos missing values não escalei os dados. devia?
os ranges dos dados são todos muito parecidos
euclidean distance para numericos e ordinais
hamming distance para nominais
devia fazer dentro de cada split? só há 9 features com KNN o resto é com a moda
SIM

NEXT STEPS:

- criar um script base para o dataset inteiro OU dataset com PCA
- extender o script para fazer feature selection (RFE) dentro de cada split

- ter sempre em atenção class weights e sample weights

---------------------------------

PERGUNTAS:
- RFE é fixe? (correlation based não ajuda muito porque nada é muito correlacionado com target e só dá para ver com numéricas)
  se sim dentro de cada fold? SIM
- KNN Imputing deve ser com dados escalados e dentro de cada fold? só há 10 features que precisam (o resto é moda e pode ir fora) SIM dados escalados mas fora dos folds
- KNeighborsClassifier só usa numerical features para ver vizinhos mais próximos? SIM
- Faz sentido incluir std deviation das performances? mediana e std SIM
- Queremos um set de teste à oarte para testar o melhor algoritmo no final? NÃO
- Queremos AUC ROC? A forma como o sklearn calcula aqui é um bocado estranho no caso de multiclass porque não há só 1 threshold etc NÃO
- Quantas configurações queremos? ~ 30 de cada

INFORMAR:
- correr o script 1 vez para cada dataset (ajustar para o que leva feature selection):
  menos risco de correr mal. demora o mesmo tempo
- algoritmo escolhidos são fixes?
- como é multiclass classification as métricas têm de ser weighted pelas classes
- há sample weights e vou usar para treinar os algoritmos
- também vou usar class_weights para treinar os algoritmos sempre que possível


- comparar estatisticamente resultados -> will-cox

--------------------------------- 24/04/2023 ------------------------------------

1. Fazer scaling antes de imputar os valores (preprocess.ipynb) DONE
2. Fazer scaling de features numéricas dentro de cada fold OK
2. Montar RFE PARA DEPOIS
3. Juntar accuracy às métricas OK
4. Juntar mediana às métricas OK
4. Mudar o número de modelos a serem criados OK
5. Incluir TPOT
6. Guardar configurações de cada modelo! (maybe escrever o dict inicial para um ficheiro)

Correr 2 vezes
- Dataset original
- Dataset com feat engineering + feat selection

-------------------------- 25/04/2023 -----------------------------

- TPOT já faz feature selection: faz sentido usar no dataset com RFE?

- No CSV final com as médias dos resultados de cada configuração, no tpot vamos ter a média
  da performance de todos os modelos que o tpot encontrou como melhores. No entanto, é preciso guardar
  num ficheiro separado qual o melhor modelo encontrado pelo tpot em todos os splits e respetiva performance

- TPOTClassifier: retorna 1 modelo (melhor modelo usando kfold)
  Queremos treinar o tpot 1 vez em cada fold meu com cv = 0/cv = 5
  Ou queremos o tpot treinado uma vez com o seu proprio KFold igual ao meu? -> problema é que não dá para costumizar folds (scaling etc.)
  Supostamente o TPOT automatiza feature selection, feature preprocessing and feature constructions por isso não seria
  preciso mete-lo dentro do kfold

  Ou seja, avaliamos todos os modelos normais com o custom KFold que escala os dados, vê as class weights e sample_weights etc
  E depois avaliamos o TPOT com a mesma instância do KFold mas sem nada de preprocessing
  Passamos apenas para o fit as sample_weights e ele depois faz lá dentro o KFold e está tudo certo

  Problema: não dá para avaliar o melhor modelo do tpot sem o testar num test set


  OU SEJA:

  1. Criar stratified kfold skf
  2. Para cada fold:
    2.1. Escalar os dados numéricos
    2.2. Guardar sample_weights do treino (usadas no método fit dos modelos)
    2.3. Guardar class_weights do treino (usadas em model evaluation metrics: f1_score, precision etc.)
    2.4. Para cada modelo:
      2.4.1. Treinar o modelo
      2.4.2. Adicionar performance do modelo e treino e val à lista de performances do modelo
  3. Calcular average de performance de cada modelo nos 10 folds (a performance com que ficamos é a de validação)

  4. Treinar TPOT em X e y (dataset inteiro) passando cv = skf como argumento
  5. Avaliar TPOT (onde???)
  
-------------------------- 25/04/2023 ------------------------------

DÚVIDAS:
1. - TPOT deve ficar fora do kfold. Basta passar o mesmo KFold para dentro dele e no final calcular o cv_score
   da melhor pipeline criada nos folds certo?
   - Podemos dizer que a Grid Search é uma forma de AutoML? Compara múltiplas configurações de modelos de forma
   'automática'.
2. TPOT já faz feature selection, por isso diria que faço apenas uma comparação final de Grid Search com e sem RFE e TPOT
3. Quando escrevi relatório disse usei 2 técnicas de AutoML (Grid Hyperparameter Search) + TPOT. Isto é correto?
   Realcei as vantagens do TPOT em relação à GridSearch. A ideia no final era mostrar que o TPOT é melhor.