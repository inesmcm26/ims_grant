- limpar o dataset e fazer visualizações: TODO visualizações com o target (URGENTE)

- ver se há outliers (IQR, DBSCAN  (atenção que há one hots)): em princípio não

- pesquisar acerca de -2 answers (URGENTE)

- lidar com imbalance target

- escolher 10 agoritmos

- fazer feature selection (PCA, correlation-based)

- 
- GRID SEARCH:
    - para cada algoritmo (FOR)
        - GRID SEARCH CV: para cada {set de parametros}: (FOR)
            - para cada fold: (FOR)
                - scaling (só se)
                - feature selection
                - (missing data)
                
                - treinar no treino
                - testar no treino ({lista de measures})
                - testar na validação ({lista de measures})
            - guardar a mediana/média (?) do algoritmo com esse set de parametros
            - ver se é melhor do que o anterior
        - retornar melhor set de parametros para algoritmo
        - escrever melhor set de parâmetros para algoritmo + train e val scores para um ficheiro
    - retornar mehor algoritmo

- DÚVIDAS:
1. teoricamente imputation deveria ser feita dentro de cada fold, mas isso iria aumentar exponencialmente o tempo necessário para fazer a grid search
tendo em conta que o número máximo de missing values por coluna roonda os 350, e sabendo que o dataset tem mais de 6800 observações, podemos
assumir que, por ser grande o suficiente, os dados não ficaram biased na validação?

2. Ver técnicas de imputation para cada variável: OK
    - drop de todas as observações com 5 ou mais missing values (-367 rows)
    - para colunas com menos de 100 missing values: central tendency limit approach (median e mode para cada caso consoante formato da distribuição)
    - para colunas com mais de 100 missing values: KNN com 5 neighbours OU moda

3. Questions not asked: o que fazer em cada caso? Em casos sem boa solução faz sentido simplesmente não usar a variável? OK

4. Feature engineering: não há variáveis muitooo correlacionadas à exceção de MATHARDSHIP. Vale a pena juntar? Ou é melhor apenas reduzir dimensionalidade usando PCA?
JUNTAR

5. Feature selection: Retirar variáveis muito correlacionadas entre sé. Vale a pena remover variáveis não relacionadas com o target ou é um risco?
Atenção que é uma análise univariada apenas. Provavelmente drop não é o ideal à partida.
ESCOLHER N FEATURES

PCA: a ideia é usar apenas os PC?

Para cada algoritmo fazer com todo o dataset, só com N features de correlation based/ Recursive best features e só com PCA

6. O que fazer em relação a imbalanced target? Usar class weights? (Se sim, como conciliar com a weight de cada variável?)
Usar Precision, Recall, PRC, AUC etc.? Escolhere ensembles?

class_weights SIM
Guardar várias metricas: Precision, recall, PRC, AUC etc.



-------------------------------------------
10/04/2023

Feature Selection: Escolher N features

- Tirei features altamente correlacionadas entre elas (3 featres foram dropped)

- Usar recursive feature elimination com decision tree (usa information gain! -> apropriado para features numericas e categóricas)

DÚVIDA: fazer à priori? ou fazer dentro de cada split?

O mais correto é dentro de cada split (só ter em conta dados de treino e não de validação)
Mas isso iria aumentar um bocado o tempo de correr o script
Correr RFE em cada split demora em média 2 min

DÚVIDA: queremos só f1 score, precision, recall, AUC certo?
weighted f1 score faz com que um peso superior seja atribuida à classe subrepresentada (pobreza máxima).
Se isto nos interessa mais do que prever class sobre representada (mínima pobreza), então talvez
faça sentido usar um weighted f1 score, ACCURACY

DÚVIDA: 
quando fiz impute dos missing values não escalei os dados. devia?
os ranges dos dados são todos muito parecidos
euclidean distance para numericos e ordinais
hamming distance para nominais
devia fazer dentro de cada split? só há 9 features com KNN o resto é com a moda

SIM

NEXT STEPS:

- criar um script base para o dataset inteiro OU dataset com PCA
- extender o script para fazer feature selection (RFE) dentro de cada split

- ter sempre em atenção class weights e sample weights

---------------------------------

PERGUNTAS:
- RFE é fixe? (correlation based não ajuda muito porque nada é muito correlacionado com target e só dá para ver com numéricas)
  se sim dentro de cada fold?
- KNN Imputing deve ser com dados escalados e dentro de cada fold? só há 10 features que precisam (o resto é moda e pode ir fora)
- KNeighborsClassifier só usa numerical features para evr viiznhos mais próximos??
- Faz sentido incluir std deviation das performances? mediana e std 
- Queremos um set de teste à oarte para testar o melhor algoritmo no final? NÃO
- Queremos AUC ROC? A forma como o sklearn calcula aqui é um bocado estranho no caso de multiclass porque não há só 1 threshold etc
- Quantas configurações queremos?

INFORMAR:
- correr o script 1 vez para cada dataset (ajustar para o que leva feature selection):
  menos risco de correr mal. demora o mesmo tempo
- algoritmo escolhidos são fixes?
- como é multiclass classification as métricas têm de ser weighted pelas classes
- há sample weights e vou usar para treinar os algoritmos
- também vou usar class_weights para treinar os algoritmos sempre que possível


- comparar estatisticamente resultados will cox